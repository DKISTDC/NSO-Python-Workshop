{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e7d7993",
   "metadata": {},
   "source": [
    "# Downloading Data\n",
    "\n",
    "## Setting up and using Globus\n",
    "\n",
    "<!--\n",
    "- login flow\n",
    "- GCP\n",
    "- paths\n",
    "- endpoints\n",
    "- monitoring jobs w/ web interface\n",
    "- download single file\n",
    "-->\n",
    "\n",
    "We've seen how to search and download the ASDF metadata files with Fido.\n",
    "However, the actual data files are distributed using [Globus](https://www.globus.org/)\n",
    "For the next portion of the workshop you will need to be running Globus Connect Personal, so follow the installation instructions for your platform [here](https://www.globus.org/globus-connect-personal) if you haven't already.\n",
    "During the setup, you will need to login to Globus.\n",
    "For this you can use your login for your institution, or alternatively you can login with Google or ORCID.\n",
    "\n",
    "Once Globus is installed and set up, you will need to run Globus Connect Personal (GCP) as described on the installation page.\n",
    "You will need to do this every time you want to download data, either through the user tools or through the Globus web app.\n",
    "When you start GCP You may also want to define the location or locations on your computer which you want Globus to have access to.\n",
    "On Linux you can do this using the `-restrict-paths` command line argument, or by editing the config file.\n",
    "On Windows and Mac OS this option is in the \"Access\" tab of the configuration options.\n",
    "Globus will only be able to transfer files onto your machine in the specified paths.\n",
    "\n",
    "### The Globus web app\n",
    "\n",
    "Many of you will already be familiar with using the [Globus web app](https://app.globus.org/) to download data.\n",
    "If you are not, you should read through the [getting started docs here](https://docs.globus.org/how-to/get-started/).\n",
    "We will not be using the web app significantly for this workshop, and generally we don't recommend downloading data this way, since the user tools are better suited to navigating the quantities of data that DKIST provides.\n",
    "However, we will be going over how to use the web app now so that we can demonstrate some of the underlying concepts.\n",
    "\n",
    "**Endpoints** (also called **Collections** in the web app) are locations registered with Globus for data transfer.\n",
    "For example, you may want to define an endpoint for both your desktop machine in the office and your laptop, so that you can download data on each depending on where you're working.\n",
    "You would then be able to transfer data directly from one to the other using Globus.\n",
    "Many institutions will have their own Globus endpoints, such as a computing cluster, that you may have access to.\n",
    "DKIST has an endpoint called \"DKIST Data Transfer\", which is where DKIST data will be made available.\n",
    "\n",
    "**Paths** are ...?\n",
    "\n",
    "To start a data transfer from one endpoint to another, go to the \"File Manager\" tab of the web app.\n",
    "Here you will find a split screen - on either side you can select an endpoint in the \"Collection\" search box.\n",
    "Select \"DKSIT Data Transfer\" on the left hand side and the endpoint corresponding to your local machine on the right.\n",
    "Then you can navigate the file system on either machine (remembering that Globus will only have access to whichever local directories you've specified).\n",
    "\n",
    "Let's demonstrate a simple file transfer by grabbing the preview movie for a dataset.\n",
    "On the right hand side in your local endpoint, navigate to a suitable place to download the movie.\n",
    "Then on the right hand side navigate to `/data/pid_1_123/BEOGN/`.\n",
    "We will use this dataset for this and some other examples later in this session.\n",
    "You should see a list of the files available in this dataset, mostly the data stored in `.fits` format.\n",
    "Select the preview movie, `BEOGN.mp4`, by clicking the checkbox next to it, then click the \"Start\" button above the file list to begin the download.\n",
    "\n",
    "You can check the progress of your transfer by going to the \"Activity\" tab, which shows both active and previous transfers.\n",
    "Various useful information is displayed here but for now the most important is whether the transfer task has failed or succeeded.\n",
    "In either case Globus will also send an email to your registered email address when the task finishes.\n",
    "Of course in this trivial example this is unneccessary, but if you're transfering a whole large dataset it will likely take some time to download and it may be useful to be notified when it's complete.\n",
    "You do not need to leave the web app open for the transfer to continue, but remember that you do need to have GCP running - so if you stop it then your data download will stop as well.\n",
    "\n",
    "If you try transfering the same file again to the same location, you will find that the task completes successfully but the file is not actually transferred.\n",
    "This is to save download time and avaoid duplication.\n",
    "\n",
    "## `dkist.Dataset` basics\n",
    "\n",
    "In DKIST data parlance, a \"dataset\" is the smallest unit of data that is searchable from the data centre, and represents a single self-contained observation [check with Stu for a better short definition here].\n",
    "The user tools represent this unit of data with the `Dataset` class.\n",
    "Within this class the data are stored as many FITS files, each containing a single frame of the observation, and an ASDF file describing how the frames relate to each other.\n",
    "For VTF data, for example, one FITS file would contain a single narrowband image in one Stokes profile at a single time.\n",
    "Since there will be very many of these files, each with their own FITS header, manually tracking and inspecting them would be unmanageable.\n",
    "The `Dataset` class combines these many files into one object, allowing you to inspect the properties and combined headers of the whole dataset.\n",
    "\n",
    "There are a few ways to construct a `Dataset` object.\n",
    "For the first we will need the ASDF file for the dataset, which we can get using `Fido` as we saw yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e05b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import dkist\n",
    "from sunpy.net import Fido, attrs as a\n",
    "import dkist.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DKIST Fido client instance\n",
    "res = Fido.search(a.dkist.Dataset('BEOGN'))\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e4ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Fido.fetch(res)\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf33402",
   "metadata": {},
   "source": [
    "Notice that the file we have downloaded is a single ASDF file, **not** the whole dataset.\n",
    "We can use this file to construct the `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dkist.Dataset.from_asdf(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca7ee8",
   "metadata": {},
   "source": [
    "Now we have a `Dataset` object which describes the shape, size and physical dimensions of the array, but doesn't yet contain any of the actual data.\n",
    "This may sound unhelpful but we'll see how it can be very powerful.\n",
    "\n",
    "First let's have a look at the basic representation of the `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ad4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41eff4",
   "metadata": {},
   "source": [
    "This tells us that we have a 4-dimensional data cube and what values the axes correspond to.\n",
    "Importantly, it not only gives us information about the *pixel* axes (the actual dimensions of the array itself), but also the *world* axes (the physical quantities related to the observation).\n",
    "It also gives us a correlation matrix showing how the pixel axes relate to the world axes.\n",
    "\n",
    "### Something?\n",
    "\n",
    "Finally the correlation matrix tells us which pixel axes correspond to which world axes.\n",
    "In this case the first three pixel axes align exactly with three of the world axes.\n",
    "However, the slit axis maps to both longitude *and* latitude, since the slit is unlikely to be aligned to either one.\n",
    "\n",
    "### Inspecting the dataset\n",
    "\n",
    "The `Dataset` object allows us to do some basic inspection of the dataset as a whole without having to download the entire thing, using the metadata in the FITS headers.\n",
    "This will save you a good amount of time and also ease the load on the DKIST servers.\n",
    "For example, we can check the seeing conditions during the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a569ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will need this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This may be useful here\n",
    "ds.meta['inventory']['headerDocumentationUrl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7371cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just look at the headers for Stokes I so there aren't 4 lots of the same values\n",
    "I_headers = ds.headers[ds.headers['DINDEX4'] == 1]\n",
    "plt.plot(I_headers['ATMOS_R0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc048ef",
   "metadata": {},
   "source": [
    "This information allows us to select the parts of the data where the seeing is good, and only download those files.\n",
    "We will see a more detailed demonstration of how to do this later.\n",
    "\n",
    "There is an important point to note about slicing the array to reduce the number of files, which is that you need to keep in mind how the data are stored across those files.\n",
    "We can see a little more information about the files with the `files` attribute of the `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c714b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b3621b",
   "metadata": {},
   "source": [
    "So in this case we can see that each FITS file contains effectively a 2D image - a single raster scan at one polarisation state - and that we have 4000 of these files to make a full 4D dataset.\n",
    "What this means is that if we look at a subset of the scan steps or polarisation states, we will reduce the number of files across which the array is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd674cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b962b653",
   "metadata": {},
   "source": [
    "First, notice that when we slice a `Dataset` like this, the output we get here shows us not just the updated array shape but also the updated dimensions.\n",
    "Because we're looking at a single polarisation state, that axis and the corresponding physical axis have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d116a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0].files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0d720",
   "metadata": {},
   "source": [
    "However, if we decide we want to look at a single wavelength, we are taking a row of pixels from every single file.\n",
    "So although we reduce the dimensions of the array, we are not reducing the number of files we need to reference - and therefore download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[:, :, 500, :].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[:, :, 500, :].files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45b096",
   "metadata": {},
   "source": [
    "### Downloading the quality report and preview movie\n",
    "\n",
    "For each dataset a quality report is produced during calibration which gives useful information about the quality of the data.\n",
    "This is accessible through the `Dataset`'s `quality_report()` method, which will download a PDF of the quality report to the base path of the dataset.\n",
    "This uses parfive underneath, which is the same library `Fido` uses, so it will return the same kind of `results` object.\n",
    "If the download has been successful, this can be treated as a list of filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ccafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr = ds.files.quality_report()\n",
    "qr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de7abc",
   "metadata": {},
   "source": [
    "This method takes the optional arguments `path` and `overwrite`.\n",
    "`path` allows you to specify a different location for the download, and `overwrite` is a boolean which tells the method whether or not to download a new copy if the file already exists.\n",
    "\n",
    "Similarly, each dataset also has a short preview movie showing the data.\n",
    "This can be downloaded in exactly the same way as the quality report but using the `preview_movie()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = ds.files.preview_movie()\n",
    "pm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed48bde",
   "metadata": {},
   "source": [
    "## Dataset and downloading\n",
    "\n",
    "<!--\n",
    "- Download from globus\n",
    "- Download whole dataset/many datasets\n",
    "- Download to remote endpoints\n",
    "-->\n",
    "\n",
    "## Downloading Data Based on Seeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf583a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sunpy.net import Fido, attrs as a\n",
    "\n",
    "import dkist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6259b8",
   "metadata": {},
   "source": [
    "Let's find a dataset with the highest average value of r0 (this is bad?).\n",
    "First we'll search for all unembargoed VISP data, as embargoed data is no use to us for this excercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b26db",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Fido.search(a.Instrument(\"VISP\"), a.dkist.Embargoed(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e53780",
   "metadata": {},
   "source": [
    "Next, since we want to use the highest average $r_0$, we can have Fido sort the results and output just the useful columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6443758",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['dkist'].sort(\"Average Fried Parameter\", reverse=True)\n",
    "res['dkist'].show(\"Dataset ID\", \"Start Time\", \"Average Fried Parameter\", \"Primary Proposal ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc404c0c",
   "metadata": {},
   "source": [
    "This gives us the dataset `BEOGN`, which happens to be the same one we have been using already.\n",
    "(So you should already have the ASDF file available on your machine but we'll go through the motions anyway for completeness.\n",
    "Fido won't re-download an existing file anyway unless told to.)\n",
    "We can download the dataset ASDF with Fido to inspect it in more detail.\n",
    "Remember that this only downloads a single ASDF file with some more metadata about the dataset, not the actual science data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd9285",
   "metadata": {
    "tags": [
     "remove-stderr"
    ]
   },
   "outputs": [],
   "source": [
    "asdf_files = Fido.fetch(res['dkist'][0], path=\"~/sunpy/data/{dataset_id}/\")\n",
    "ds = dkist.Dataset.from_asdf(asdf_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5945e",
   "metadata": {},
   "source": [
    "Now that we have access to the FITS headers we can inspect the $r_0$ more closely, just as we did in the previous session.\n",
    "Remember that `DINDEX4` is the Stokes index, so we can plot the $r_0$ for just Stokes I like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58aec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds.headers[ds.headers[\"DINDEX4\"] == 1][\"ATMOS_R0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559b45bd",
   "metadata": {},
   "source": [
    "Now let's slice down our dataset based on the first frame where $r_0$ is high:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e6973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select headers for only frames with bad r0\n",
    "bad_headers = ds.headers[ds.headers[\"ATMOS_R0\"] > 1]\n",
    "\n",
    "# Make sure headers are sorted by date\n",
    "bad_headers.sort(\"DATE-AVG\")\n",
    "\n",
    "# Slice up to the index of the first bad frame\n",
    "sds = ds[0, :bad_headers[0][\"DINDEX3\"]-1, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426293d",
   "metadata": {},
   "source": [
    "We can now download only these files, remember you need globus-connect-personal running for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72d2b2",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "sds.files.download(path=\"~/sunpy/data/{dataset_id}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883f15a",
   "metadata": {},
   "source": [
    "Now let's plot the Stokes I data at some wavelength:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46094204",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "ds[0, :, 466, :].plot(plot_axes=['x', 'y'], aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbee66f",
   "metadata": {},
   "source": [
    "You will notice that a lot of it is missing.\n",
    "This is because we have deliberately only downloaded those frames with an acceptably low $r_0$.\n",
    "You may also notice though, that the `Dataset` object continues to function normally without the rest of the data.\n",
    "When we try to access the data, if the file is missing then `Dataset` fills in the corresponding portions of the array with NaNs.\n",
    "\n",
    "Since the seeing is bad for a significant contiguous portion of the data, we may simply want to discount that part and look only at the useful data.\n",
    "In this case we can use the sub-dataset we made earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666bf147",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "sds[:, 466, :].plot(plot_axes=['x', 'y'], aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb76268",
   "metadata": {},
   "source": [
    "Or of course we can make any arbitrary slice to look at whatever subset of the data we prefer.\n",
    "\n",
    "\n",
    "## Plotting again\n",
    "\n",
    "<!--\n",
    "- put data in the plot\n",
    "- animations\n",
    "- single image w/ coord overlay\n",
    "- line plots\n",
    "- plotting (ndcube)\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
